{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv1\n",
    "\n",
    "Adapted from https://www.kaggle.com/code/vexxingbanana/yolov1-from-scratch-pytorch/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import time\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './fruit/train_zip/train'\n",
    "test_dir = './fruit/test_zip/test'\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"mps\"\n",
    "BATCH_SIZE = 16 # 64 in the original paper but can use too much resources\n",
    "DROPOUT = 0.0 # Original paper uses 0.5\n",
    "LAST_HIDDEN_SIZE = 496 # Original paper uses 4096\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 0\n",
    "LOAD_MODEL = False\n",
    "MODEL_FILE = \"model.pth\"\n",
    "\n",
    "class_names = ['apple', 'banana', 'orange']\n",
    "class_colors = [(255,0,0), (0,255,0), (0,0,255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    #Tuple: (kernel_size, number of filters, strides, padding)\n",
    "    (7, 64, 2, 3),\n",
    "    #\"M\" = Max Pool Layer\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    #List: [(tuple), (tuple), how many times to repeat]\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    #Doesnt include fc layers\n",
    "]\n",
    "\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "    \n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0] #Tuple\n",
    "                conv2 = x[1] #Tuple\n",
    "                repeats = x[2] #Int\n",
    "                \n",
    "                for _ in range(repeats):\n",
    "                    layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
    "                    layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
    "                    in_channels = conv2[1]\n",
    "                    \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(nn.Flatten(), nn.Linear(1024 * S * S, LAST_HIDDEN_SIZE), nn.Dropout(DROPOUT), nn.LeakyReLU(0.1), nn.Linear(LAST_HIDDEN_SIZE, S * S * (C + B * 5)))\n",
    "    \n",
    "# Instantiate the model and check the parameters\n",
    "model = YoloV1(split_size=7, num_boxes=2, num_classes=3)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(loop):\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
    "        out = model(images)\n",
    "        loss = loss_fn(out, targets)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, C=3):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = 2\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        #Output structure: [class scores, box1, box2]\n",
    "        #Box structure: [confidence, x, y, w, h]\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        i1obj_i = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #       BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = i1obj_i * (\n",
    "            (\n",
    "                bestbox * predictions[..., self.C + 6:self.C + 10]\n",
    "                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = i1obj_i * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        # Take sqrt of width, height of boxes\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #       OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(i1obj_i * pred_box),\n",
    "            torch.flatten(i1obj_i * iou_maxes),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #       NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - i1obj_i) * predictions[..., self.C:self.C + 1], start_dim=1),\n",
    "            torch.flatten((1 - i1obj_i) * target[..., self.C:self.C + 1], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - i1obj_i) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n",
    "            torch.flatten((1 - i1obj_i) * torch.zeros_like(target[..., self.C:self.C + 1]), start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #       CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(i1obj_i * predictions[..., :self.C], end_dim=-2,),\n",
    "            torch.flatten(i1obj_i * target[..., :self.C], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # fourth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n",
    "\n",
    "def main(model):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(MODEL_FILE), model, optimizer)\n",
    "\n",
    "    summary(model)\n",
    "\n",
    "    train_dataset = FruitImagesDataset(\n",
    "        transform=transform,\n",
    "        files_dir=train_dir\n",
    "    )\n",
    "\n",
    "    test_dataset = FruitImagesDataset(\n",
    "        transform=transform,\n",
    "        files_dir=test_dir\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader_unshuffled = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "        \n",
    "        pred_boxes, target_boxes, _ = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_boxes, target_boxes, images = get_bboxes(\n",
    "                test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "            )\n",
    "            mean_avg_prec = mean_average_precision(\n",
    "                pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "            )\n",
    "            print(f\"Test mAP: {mean_avg_prec}\")\n",
    "        model.train()\n",
    "\n",
    "        \n",
    "        scheduler.step(mean_avg_prec)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=MODEL_FILE)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(1):\n",
    "            start = time.time()\n",
    "            pred_boxes, target_boxes, images = get_bboxes(\n",
    "                train_loader_unshuffled, model, iou_threshold=0.5, threshold=0.4\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "            # Visualize\n",
    "            for image_i in range(len(images)):\n",
    "                # Find all boxes related to image_i\n",
    "                pred_boxes_image_i = [box for box in pred_boxes if box[0] == image_i]\n",
    "                target_boxes_image_i = [box for box in target_boxes if box[0] == image_i]\n",
    "                image_with_boxes = visualize_boxes(images[image_i].cpu().numpy().transpose(1,2,0)[:, :, ::-1], pred_boxes_image_i, class_names, class_colors)\n",
    "                # Export the image\n",
    "                os.makedirs(\"./train_pred\", exist_ok=True)\n",
    "                cv2.imwrite(f\"./train_pred/{image_i}_pred.jpg\", image_with_boxes*255)\n",
    "\n",
    "            mean_avg_prec = mean_average_precision(\n",
    "                pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "            )\n",
    "            print(f\"Train mAP: {mean_avg_prec}; Time: {end - start} for {len(images)} images\")\n",
    "\n",
    "        for _ in range(1):\n",
    "            start = time.time()\n",
    "            pred_boxes, target_boxes, images = get_bboxes(\n",
    "                test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "            # Visualize\n",
    "            for image_i in range(len(images)):\n",
    "                # Find all boxes related to image_i\n",
    "                pred_boxes_image_i = [box for box in pred_boxes if box[0] == image_i]\n",
    "                target_boxes_image_i = [box for box in target_boxes if box[0] == image_i]\n",
    "                image_with_boxes = visualize_boxes(images[image_i].cpu().numpy().transpose(1,2,0)[:, :, ::-1], pred_boxes_image_i, class_names, class_colors)\n",
    "                # Export the image\n",
    "                os.makedirs(\"./test_pred\", exist_ok=True)\n",
    "                cv2.imwrite(f\"./test_pred/{image_i}_pred.jpg\", image_with_boxes*255)\n",
    "\n",
    "            mean_avg_prec = mean_average_precision(\n",
    "                pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "            )\n",
    "            print(f\"Test mAP: {mean_avg_prec}; Time: {end - start} for {len(images)} images\")\n",
    "\n",
    "main(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions():\n",
    "\n",
    "    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    load_checkpoint(torch.load(MODEL_FILE), model, optimizer)\n",
    "\n",
    "    test_dataset = FruitImagesDataset(\n",
    "        transform=transform, \n",
    "        files_dir=test_dir\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "        \n",
    "    for epoch in range(1):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_boxes, target_boxes, _ = get_bboxes(\n",
    "                test_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "            )\n",
    "\n",
    "            mean_avg_prec = mean_average_precision(\n",
    "                pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "            )\n",
    "            print(f\"Test mAP: {mean_avg_prec}\")\n",
    "\n",
    "predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzhackbootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
